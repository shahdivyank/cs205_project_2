{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS205 Project 2\n",
    "### Divyank Shah (dshah048)\n",
    "### 14th June 2024\n",
    "\n",
    "Github Link: https://github.com/shahdivyank/cs205_project_2\n",
    "\n",
    "In order to implement this project, I referenced the following sites/resouces:\n",
    "\n",
    "1. Numpy Documentation: https://numpy.org/doc/stable/index.html \n",
    "2. Pandas Documentation: https://pandas.pydata.org/docs/\n",
    "3. SkLearn Documentation: https://scikit-learn.org/stable/ \n",
    "4. Slides (Project 2 Briefing) + Example report provided\n",
    "\n",
    "I affirm that I did not use ChatGPT or similar to write the code or text in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The following projects attempts to depicts the forward and backward feature selection algorithms on varying sized datasets with numerous features. The small dataset consists of 12 features and 500 datapoints while the large dataset consists of 50 features and 5000 datapoints. \n",
    "\n",
    "The following report dives into the project assigned in CS 205: Artificial Intelligence by Professor Keogh in Spring 2024 at the University of California, Riverside. The write up includes information about the implementation, various bugs, and optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/zydtttq12xq05qpxw71t17200000gn/T/ipykernel_44614/1190984141.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was assigned Small_40 and Large_15 as per the project description requirements\n",
    "\n",
    "SMALL_FILENAME = \"./data/CS205_small_Data__40.txt\"\n",
    "LARGE_FILENAME = \"./data/CS205_large_Data__15.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse both data files into dataframes using the read_fwf function which takes in a fixed width based data file\n",
    "# https://stackoverflow.com/questions/68871370/parse-problematic-fixed-width-text-file-to-a-pandas-dataframe\n",
    "# https://pandas.pydata.org/pandas-docs/version/1.2.0/reference/api/pandas.read_fwf.html\n",
    "\n",
    "small_df = pd.read_fwf(SMALL_FILENAME, header = None)\n",
    "large_df = pd.read_fwf(LARGE_FILENAME, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the X (features) and Y (labels) for each small and large dataset and convert to numpy for faster execution\n",
    "\n",
    "X_SMALL = small_df.iloc[:, 1:].to_numpy()\n",
    "Y_SMALL = small_df.iloc[:, 0].to_numpy()\n",
    "\n",
    "X_LARGE = large_df.iloc[:, 1:].to_numpy()\n",
    "Y_LARGE = large_df.iloc[:, 0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to take the distance matrix provided and find the smallest distance. Ensures that it does not return itself, \n",
    "# by setting its own distance to infinity and reverting it back afterwards \n",
    "\n",
    "def nearest_neighbor(distances, index):\n",
    "    distances[index][index] = np.inf\n",
    "    shortest_distance_index = np.argmin(distances[index])\n",
    "    distances[index][index] = 0\n",
    "\n",
    "    return shortest_distance_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find the distance between all datapoints with selected features.\n",
    "# Based on the pairwise distance matrix, it will get the nearest neighbor and the true label. Aggregating this information\n",
    "# is then used to calculate the accuracy for the given set of features.\n",
    "\n",
    "def calculate_accuracy(X, Y, features):\n",
    "    correct_classified = 0\n",
    "\n",
    "    distances = pairwise_distances(X[:, features])\n",
    "\n",
    "    for datapoint_index in range(X.shape[0]):\n",
    "        nearest_neighbor_index = nearest_neighbor(distances, datapoint_index)\n",
    "        classified_class = Y[nearest_neighbor_index]\n",
    "        correct_class = Y[datapoint_index]\n",
    "\n",
    "        if classified_class == correct_class:\n",
    "            correct_classified += 1\n",
    "\n",
    "    return correct_classified / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feature Selection\n",
    "\n",
    "The forward selection algorithm is a method of starting from no selected features and adding a single feature each iteration. The feature with the highest accuracy is then chosen and inputed into a local maxima features list. This process repeats until all features have been selected. During this process, the global maxima is also kept track of and ultimately reported at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The primary forward feature selection algortithm. Originally starting with an empty feature list and\n",
    "# adding features iteratively and choosing the path with the highest accuracy and expanding from there.\n",
    "# Global and local maximums are tracked to ensure the most optimal solution is found after finishing the\n",
    "# algorithms.\n",
    "\n",
    "def forward_feature_selection(X, Y):\n",
    "    current_features = []\n",
    "    global_accuracy = -np.inf\n",
    "    global_features = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        print(f\"On the {i}th level of the search tree\")\n",
    "        feature_to_add = None\n",
    "        best_accuracy = 0.0\n",
    "\n",
    "        for k in range(X.shape[1]):\n",
    "            if k not in current_features:\n",
    "                accuracy = calculate_accuracy(X, Y, current_features + [k])\n",
    "                print(f'- Adding the {k + 1} feature with accuracy: {accuracy}')\n",
    "            \n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    feature_to_add = k\n",
    "            \n",
    "        if best_accuracy >= global_accuracy:\n",
    "            global_accuracy = best_accuracy\n",
    "            global_features.append(feature_to_add)\n",
    "        else:\n",
    "            print(\"\\n**Warning, Accuracy has decreased! Continuing search in case of local maxima**\")\n",
    "\n",
    "        current_features.append(feature_to_add)\n",
    "        print(f'\\nOn level {i}, added feature {feature_to_add} to current set. With current features: {current_features} and local maximum accuracy {best_accuracy}\\n')\n",
    "    print(f\"\\n\\nFinished. The best subset is {global_features} (indexes) {[x + 1 for x in global_features]} (features), which has an accuracy of {global_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Feature Selection\n",
    "\n",
    "The backward feature algorithm operates by initially selecting all features and then removing exactly one feature. The feature set with n - 1 features with the highest accuracy is reduced further by another feature until there exists only a single feature. During the various iterations the local and global maxima are tracked and reported at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The primary backward feature selection algortithm. Originally starting with an full feature list and\n",
    "# removing features iteratively and choosing the path with the highest accuracy and expanding from there.\n",
    "# Global and local maximums are tracked to ensure the most optimal solution is found after finishing the\n",
    "# algorithms.\n",
    "\n",
    "def backward_feature_selection(X, Y):\n",
    "    current_features = [i for i in range(X.shape[1])]\n",
    "    global_features = [i for i in range(X.shape[1])]\n",
    "\n",
    "    global_accuracy = calculate_accuracy(X, Y, current_features)\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        print(f\"On the {i}th level of the search tree\")\n",
    "        feature_to_remove = None\n",
    "        best_accuracy = 0.0\n",
    "\n",
    "        for k in range(X.shape[1]):\n",
    "            if k in current_features and len(current_features) > 1:\n",
    "                accuracy = calculate_accuracy(X, Y, list(set(current_features) - set([k])))\n",
    "\n",
    "                print(f'- Removing the {k} feature with accuracy: {accuracy}')\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    feature_to_remove = k\n",
    "            \n",
    "        if best_accuracy >= global_accuracy and feature_to_remove in global_features:\n",
    "            global_accuracy = best_accuracy\n",
    "            global_features.remove(feature_to_remove)\n",
    "        else:\n",
    "            print(\"\\n**Warning, Accuracy has decreased! Continuing search in case of local maxima**\")\n",
    "\n",
    "        if feature_to_remove is not None:\n",
    "            current_features.remove(feature_to_remove)\n",
    "        print(f'\\nOn level {i}, removed feature {feature_to_remove} to current set. With current features: {current_features} and local maximum accuracy {best_accuracy}\\n')\n",
    "    print(f\"\\n\\nFinished. The best subset is {global_features} (indexes) {[x + 1 for x in global_features]} (features), which has an accuracy of {global_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "# Testbench for small dataset on SMALL_40 with forward feature selection.\n",
    "\n",
    "forward_feature_selection(X_SMALL, Y_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "# Testbench for small dataset on SMALL_40 with backward feature selection.\n",
    "\n",
    "backward_feature_selection(X_SMALL, Y_SMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "# Testbench for small dataset on LARGE_15 with forward feature selection.\n",
    "\n",
    "forward_feature_selection(X_LARGE, Y_LARGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "\n",
    "# Testbench for small dataset on LARGE_15 with backward feature selection.\n",
    "\n",
    "backward_feature_selection(X_LARGE, Y_LARGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using the traces provided in the `/results` folder, the accuracies across various iterations, the accuracy of removing or adding a feature can be seen and analyzed further if desired. \n",
    "\n",
    "### Small Dataset 40\n",
    "Results for the small dataset were consistent were at least 2 of the expected 3 features were selected. Due to the initial separation of features and labels into X and Y, the features shown are the feature indexes which is off by one relative to the true feature number. Results for forward and backward feature selection are consistent however they appear in a different order. \n",
    "\n",
    "### Large Dataset 15\n",
    "Results for the large dataset were consistent were at least 2 of the expected 3 features were selected in forward feature selection. Due to the initial separation of features and labels into X and Y, the features shown are the feature indexes which is off by one relative to the true feature number. Backwards feature selection does appear to have a bug which is only apparent when testing with larger datasets. It outputs to have multiple variables chosen when in reality it should have had only 2-3 variables. However, the expected features are included, accounting for the off by 1 index due to the X and Y split. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
